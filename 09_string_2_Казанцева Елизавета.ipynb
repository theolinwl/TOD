{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Obtaining dependency information for python-Levenshtein from https://files.pythonhosted.org/packages/72/8e/559c539e76bc0b1defec3da39a047fe151258efc9b215bf41db41e2c7922/python_Levenshtein-0.25.1-py3-none-any.whl.metadata\n",
      "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Levenshtein==0.25.1 (from python-Levenshtein)\n",
      "  Obtaining dependency information for Levenshtein==0.25.1 from https://files.pythonhosted.org/packages/47/19/4528246e25bb79fa8d4adae6640251c613f05eb310d79307d1ac53c7bf28/Levenshtein-0.25.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading Levenshtein-0.25.1-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein==0.25.1->python-Levenshtein)\n",
      "  Obtaining dependency information for rapidfuzz<4.0.0,>=3.8.0 from https://files.pythonhosted.org/packages/04/10/2c0ef45d4ace8dde87cfb91e48fb5c9976f8e01a57eb3230d90b82801dc5/rapidfuzz-3.9.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading rapidfuzz-3.9.3-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Downloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading Levenshtein-0.25.1-cp311-cp311-win_amd64.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.4 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/98.4 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 61.4/98.4 kB 656.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 98.4/98.4 kB 801.1 kB/s eta 0:00:00\n",
      "Downloading rapidfuzz-3.9.3-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.7 MB 4.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.3/1.7 MB 3.4 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.4/1.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.3/1.7 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 6.6 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.25.1 python-Levenshtein-0.25.1 rapidfuzz-3.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велИчайшим усилеем выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "читайте слова из файл и и запишите их в список и и заданной предложении исправьте все опечатки заменив слова с печатками на ближайшие в смысле расстояния рубинштейна к ним слова из списка и читайте что в слове есть опечатки если данное слово не содержится в списке и\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# cчитываем слова из файла\n",
    "with open(\"data\\litw-win.txt\", \"r\") as f:\n",
    "    words = [line.split()[1] for line in f]\n",
    "\n",
    "    \n",
    "# исправление опечаток\n",
    "def correct_word(word, words):\n",
    "    if word in words:  # если слово в words, значит исправлять не надо, просто возвращаем слово\n",
    "        return word\n",
    "    else:\n",
    "        # находим ближайшее слово по расстоянию Левенштейна\n",
    "        min_distance = float('inf')\n",
    "        corrected_word = None\n",
    "        for w in words:\n",
    "            distance = Levenshtein.distance(word, w)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                corrected_word = w\n",
    "        return corrected_word\n",
    "\n",
    "corrected_sentence = \" \".join([correct_word(word, words) for word in text.split()])\n",
    "\n",
    "print(corrected_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "стемминг удаляет суффиксы и префиксы из слов\n",
    "\n",
    "лемматизация приводит слова к их основной форме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['с', 'величайшим', 'усилием', 'выбравшись', 'из', 'потока', 'убегающих', 'людей', 'кутузов', 'со', 'свитой', 'уменьшившейся', 'вдвое', 'поехал', 'на', 'звуки', 'выстрелов', 'русских', 'орудий']\n"
     ]
    }
   ],
   "source": [
    "#Разбиение текста на слова:\n",
    "import nltk\n",
    "\n",
    "words = nltk.word_tokenize(corrected_sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0]\n",
      " [0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 2 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1]]\n",
      " \n",
      "[[1 1 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1\n",
      "  0]\n",
      " [0 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 0\n",
      "  0]\n",
      " [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0\n",
      "  1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Токенизировать текст на предложения на русском языке\n",
    "texti = sent_tokenize(text, language=\"russian\")\n",
    "\n",
    "# Создать корпус из токенизированных предложений\n",
    "corpus = [i for i in texti]\n",
    "\n",
    "# Создать экземпляр CountVectorizer\n",
    "vectorizer1 = CountVectorizer()\n",
    "\n",
    "# Создать экземпляр CountVectorizer с анализатором уровня слова и диапазоном n-грамм (2, 2)\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "\n",
    "# Преобразовать корпус в матрицу признаков с помощью CountVectorizer\n",
    "X = vectorizer1.fit_transform(corpus)\n",
    "\n",
    "# Получить имена признаков из CountVectorizer\n",
    "vectorizer1.get_feature_names_out()\n",
    "\n",
    "# Вывести матрицу признаков\n",
    "print(X.toarray())\n",
    "print(\" \")\n",
    "\n",
    "# Преобразовать корпус в матрицу признаков с помощью CountVectorizer\n",
    "X = vectorizer2.fit_transform(corpus)\n",
    "\n",
    "# Получить имена признаков из CountVectorizer\n",
    "vectorizer2.get_feature_names_out()\n",
    "\n",
    "# Вывести матрицу признаков\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "нету preprocessed_descriptions.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пар -> между: 5\n",
      "Сгенерируйте -> пар: 11\n",
      "редактирования -> посчитайте: 12\n",
      "и -> выбранных: 9\n",
      "расстояние -> ними: 9\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import Levenshtein\n",
    "\n",
    "# Сгенерировать 5 пар случайных слов\n",
    "pairs = [(random.choice(words), random.choice(words)) for i in range(5)]\n",
    "\n",
    "# Посчитать расстояние редактирования для каждой пары\n",
    "distances = [Levenshtein.distance(pair[0], pair[1]) for pair in pairs]\n",
    "\n",
    "# Вывести пары слов и расстояния редактирования\n",
    "for pair, distance in zip(pairs, distances):\n",
    "    print(f\"{pair[0]} -> {pair[1]}: {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['солнце',\n",
       " 'солнц',\n",
       " 'солне',\n",
       " 'сонце',\n",
       " 'солнца',\n",
       " 'солнцу',\n",
       " 'солнцы',\n",
       " 'солнцем',\n",
       " 'соне',\n",
       " 'солн']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def find_k_nearest_neighbors(word, words, k):\n",
    "    # Посчитать расстояния Левенштейна между заданным словом и всеми словами из списка\n",
    "    distances = [Levenshtein.distance(word, w) for w in words]\n",
    "\n",
    "    # Отсортировать слова по расстоянию Левенштейна\n",
    "    sorted_words = sorted(zip(distances, words), key=lambda x: x[0])\n",
    "\n",
    "    # Вернуть k ближайших соседей\n",
    "    return [w for d, w in sorted_words[:k]]\n",
    "\n",
    "find_k_nearest_neighbors('солнце', words, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Загрузка данных из файла\n",
    "file_path = 'Пока нет файла. Увы'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Выбор случайных 5 рецептов\n",
    "random_recipes = df.sample(n=5, random_state=42)\n",
    "\n",
    "# Инициализация векторизатора TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Векторизация описаний выбранных рецептов\n",
    "tfidf_matrix = vectorizer.fit_transform(random_recipes['description'])\n",
    "\n",
    "# Преобразование в DataFrame для удобства\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=random_recipes['title'], columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "tfidf_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Вычисление косинусного расстояния и создание таблицы\n",
    "cosine_distances = pd.DataFrame(index=random_recipes['title'], columns=random_recipes['title'])\n",
    "\n",
    "for i, title1 in enumerate(random_recipes['title']):\n",
    "    for j, title2 in enumerate(random_recipes['title']):\n",
    "        if i != j:\n",
    "            cosine_distances.at[title1, title2] = cosine(tfidf_df.loc[title1], tfidf_df.loc[title2])\n",
    "        else:\n",
    "            cosine_distances.at[title1, title2] = 0.0\n",
    "\n",
    "cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск пары рецептов с минимальным косинусным расстоянием (кроме диагонали)\n",
    "min_distance = float('inf')\n",
    "min_pair = None\n",
    "\n",
    "for i, title1 in enumerate(random_recipes['title']):\n",
    "    for j, title2 in enumerate(random_recipes['title']):\n",
    "        if i != j and cosine_distances.at[title1, title2] < min_distance:\n",
    "            min_distance = cosine_distances.at[title1, title2]\n",
    "            min_pair = (title1, title2)\n",
    "\n",
    "min_distance, min_pair"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
